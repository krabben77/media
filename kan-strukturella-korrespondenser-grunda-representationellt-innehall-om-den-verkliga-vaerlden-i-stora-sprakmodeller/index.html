<!DOCTYPE html><html lang="sv"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Kan strukturella korrespondenser grunda representationellt innehåll om den verkliga världen i stora språkmodeller? - Avpoxlat</title><meta name="description" content="==Start på översättning av sida 1== Kan strukturella korrespondenser grunda representationellt innehåll om den verkliga världen i stora språkmodeller? Iwan Williams (ORCID: 0000-0003-0582-0983) &#105;&#x77;&#97;&#x6e;&#x2e;&#x72;&#46;&#119;&#105;&#108;&#x6c;&#x69;&#97;&#x6d;&#x73;&#x40;&#103;&#109;&#x61;&#x69;&#108;&#x2e;&#99;&#x6f;&#x6d; Centre&hellip;"><meta name="generator" content="Publii Open-Source CMS for Static Site"><script type="text/javascript">var _paq = window._paq = window._paq || [];
					_paq.push(['trackPageView']);
					_paq.push(['enableLinkTracking']);
					(function() {
					  var u="/";
					  _paq.push(['setTrackerUrl', u+'matomo.php']);
					  _paq.push(['setSiteId', '']);
					  var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
					  g.async=true; g.src='/matomo.js'; s.parentNode.insertBefore(g,s);
					})();</script><link rel="canonical" href="https://avpixlat.xyz/kan-strukturella-korrespondenser-grunda-representationellt-innehall-om-den-verkliga-vaerlden-i-stora-sprakmodeller/"><link rel="alternate" type="application/atom+xml" href="https://avpixlat.xyz/feed.xml" title="Avpoxlat - RSS"><link rel="alternate" type="application/json" href="https://avpixlat.xyz/feed.json" title="Avpoxlat - JSON"><meta property="og:title" content="Kan strukturella korrespondenser grunda representationellt innehåll om den verkliga världen i stora språkmodeller?"><meta property="og:image" content="https://avpixlat.xyz/media/website/AI-Taking-Over-the-World-2.webp"><meta property="og:image:width" content="1792"><meta property="og:image:height" content="1024"><meta property="og:site_name" content="Avpoxlat"><meta property="og:description" content="==Start på översättning av sida 1== Kan strukturella korrespondenser grunda representationellt innehåll om den verkliga världen i stora språkmodeller? Iwan Williams (ORCID: 0000-0003-0582-0983) &#105;&#x77;&#97;&#x6e;&#x2e;&#x72;&#46;&#119;&#105;&#108;&#x6c;&#x69;&#97;&#x6d;&#x73;&#x40;&#103;&#109;&#x61;&#x69;&#108;&#x2e;&#99;&#x6f;&#x6d; Centre&hellip;"><meta property="og:url" content="https://avpixlat.xyz/kan-strukturella-korrespondenser-grunda-representationellt-innehall-om-den-verkliga-vaerlden-i-stora-sprakmodeller/"><meta property="og:type" content="article"><link rel="preload" href="https://avpixlat.xyz/assets/dynamic/fonts/jetbrainsmono/jetbrainsmono.woff2" as="font" type="font/woff2" crossorigin><link rel="preload" href="https://avpixlat.xyz/assets/dynamic/fonts/jetbrainsmono/jetbrainsmono-italic.woff2" as="font" type="font/woff2" crossorigin><link rel="stylesheet" href="https://avpixlat.xyz/assets/css/style.css?v=9ac3045b1a727c7938c187aa0847fb44"><script type="application/ld+json">{"@context":"http://schema.org","@type":"Article","mainEntityOfPage":{"@type":"WebPage","@id":"https://avpixlat.xyz/kan-strukturella-korrespondenser-grunda-representationellt-innehall-om-den-verkliga-vaerlden-i-stora-sprakmodeller/"},"headline":"Kan strukturella korrespondenser grunda representationellt innehåll om den verkliga världen i stora språkmodeller?","datePublished":"2025-06-30T14:39+02:00","dateModified":"2025-06-30T14:57+02:00","image":{"@type":"ImageObject","url":"https://avpixlat.xyz/media/website/AI-Taking-Over-the-World-2.webp","height":1024,"width":1792},"description":"==Start på översättning av sida 1== Kan strukturella korrespondenser grunda representationellt innehåll om den verkliga världen i stora språkmodeller? Iwan Williams (ORCID: 0000-0003-0582-0983) &#105;&#x77;&#97;&#x6e;&#x2e;&#x72;&#46;&#119;&#105;&#108;&#x6c;&#x69;&#97;&#x6d;&#x73;&#x40;&#103;&#109;&#x61;&#x69;&#108;&#x2e;&#99;&#x6f;&#x6d; Centre&hellip;","author":{"@type":"Person","name":"Redaktören","url":"https://avpixlat.xyz/authors/redaktoeren/"},"publisher":{"@type":"Organization","name":"Redaktören","logo":{"@type":"ImageObject","url":"https://avpixlat.xyz/media/website/AI-Taking-Over-the-World-2.webp","height":1024,"width":1792}}}</script><noscript><style>img[loading] {
                    opacity: 1;
                }</style></noscript><style>.extlink::after {
				background-color: currentColor;
				content: "";
				display: inline-block;
				height: 16px;
				margin-left: 5px;
				position: relative;
				top: 0px;
				width: 16px;
			}
		
					.extlink.extlink-icon-1::after {
						mask-image: url("data:image/svg+xml;utf8,<svg viewBox='0 0 24 24' fill='none' stroke='%23000000' stroke-width='2' stroke-linecap='round' stroke-linejoin='round' xmlns='http://www.w3.org/2000/svg'><path d='M15 3h6v6'/><path d='M10 14 21 3'/><path d='M18 13v6a2 2 0 0 1-2 2H5a2 2 0 0 1-2-2V8a2 2 0 0 1 2-2h6'/></svg>");
						mask-repeat: no-repeat;
						mask-size: contain;
					}</style></head><body class="post-template"><div class="container container--center"><header class="header"><div class="header__logo"><a class="logo" href="https://avpixlat.xyz/"><img src="https://avpixlat.xyz/media/website/AI-Taking-Over-the-World-2.webp" alt="Avpoxlat" width="1792" height="1024"></a></div><nav class="navbar js-navbar"><button class="navbar__toggle js-toggle" aria-label="Menu" aria-haspopup="true" aria-expanded="false"><span class="navbar__toggle-box"><span class="navbar__toggle-inner">Menu</span></span></button><ul class="navbar__menu"><li class="has-submenu"><a href="https://avpixlat.xyz/" target="_self" aria-haspopup="true">index</a><ul class="navbar__submenu level-2" aria-hidden="true"><li><a href="https://avpixlat.xyz/tags/ai/" target="_self">AI</a></li><li><a href="https://avpixlat.xyz/tags/medvetandet/" target="_self">Medvetandet</a></li><li><a href="https://avpixlat.xyz/tags/filosofi/" target="_self">Filosofi</a></li></ul></li></ul></nav></header><main class="content"><article class="post"><header><h1 class="post__title">Kan strukturella korrespondenser grunda representationellt innehåll om den verkliga världen i stora språkmodeller?</h1><div class="post__meta"><time datetime="2025-06-30T14:39" class="post__date">juni 30, 2025 </time><span class="post__author"><a href="https://avpixlat.xyz/authors/redaktoeren/" class="feed__author">Redaktören</a></span></div><div class="post__tags"><a href="https://avpixlat.xyz/tags/ai/" class="invert">AI</a></div></header><div class="post__entry"><hr><p>==Start på översättning av sida 1== <strong>Kan strukturella korrespondenser grunda representationellt innehåll om den verkliga världen i stora språkmodeller?</strong></p><p><strong>Iwan Williams (ORCID: 0000-0003-0582-0983)</strong> <a href="mailto:&#105;&#x77;&#97;&#x6e;&#x2e;&#x72;&#46;&#119;&#105;&#108;&#x6c;&#x69;&#97;&#x6d;&#x73;&#x40;&#103;&#109;&#x61;&#x69;&#108;&#x2e;&#99;&#x6f;&#x6d;">&#105;&#x77;&#97;&#x6e;&#x2e;&#x72;&#46;&#119;&#105;&#108;&#x6c;&#x69;&#97;&#x6d;&#x73;&#x40;&#103;&#109;&#x61;&#x69;&#108;&#x2e;&#99;&#x6f;&#x6d;</a> Centre for Philosophy of AI, University of Copenhagen</p><p><strong>Sammanfattning:</strong></p><p>Stora språkmodeller (LLM:er) som GPT-4 producerar övertygande svar på ett brett spektrum av prompter. Men deras representationella förmågor är osäkra. Många LLM:er har ingen direkt kontakt med den utomspråkliga verkligheten: deras indata, utdata och träningsdata består enbart av text, vilket väcker frågorna (1) kan LLM:er representera <em>någonting</em> och (2) i så fall, <em>vad</em>? I denna artikel utforskar jag vad som skulle krävas för att besvara dessa frågor enligt en redogörelse för representation baserad på <em>strukturell korrespondens</em>, och gör en inledande undersökning av bevisen. Jag argumenterar för att enbart existensen av strukturella korrespondenser mellan LLM:er och världsliga entiteter är otillräcklig för att grunda representation av dessa entiteter. Men om dessa strukturella korrespondenser spelar en lämplig roll – de utnyttjas på ett sätt som förklarar framgångsrik uppgiftsprestation – då skulle de kunna grunda innehåll om den verkliga världen. Detta kräver att man övervinner en utmaning: textbundenheten hos LLM:er verkar vid första anblicken förhindra dem från att engagera sig i rätt sorts uppgifter.</p><p><strong>1. Introduktion</strong></p><p>Stora språkmodeller (LLM:er) är artificiella system utformade för att modellera, bearbeta och/eller generera naturligt språk. Historiskt sett inkluderade dessa system rent statistiska modeller, men moderna LLM:er är djupa artificiella neuronnät tränade via maskininlärning. När en LLM väl är tränad kan den implementeras för olika ändamål, såsom i chattbotar och personliga assistenter, eller för översättning, sentimentanalys och dokumentgranskning.</p><p>==Slut på översättning av sida 1==</p><p>==Start på översättning av sida 2== Den onekligen imponerande prestandan hos LLM:er på en mängd olika uppgifter väcker angelägna frågor om deras förmågor och mekanismerna som ligger bakom dessa förmågor. Forskare har till exempel brottats med frågorna om huruvida LLM:er förstår språk (Bender &amp; Koller, 2020; Mitchell &amp; Krakauer, 2022), om de besitter begrepp (Butlin, 2023) eller i vilken utsträckning de besitter en medvetandeteori (Kosinski, 2024; Ullman 2023).</p><p>Denna artikel fokuserar på de representationella förmågorna hos LLM:er. Förlitar sig LLM:er på representationer? Om så, vad representerar dessa representationer? Mycket forskning inom AI – till exempel studier som använder sondningsklassificerare (Belinkov, 2022) och metoder för att “redigera” modellers representationer (Hernandez et al., 2024; Meng et al., 2022) – antar att en representationell lins är lämplig. Men en nyckelfråga är om LLM:er kan representera verkliga världsentiteter, eller bara “ytligt” lingvistiskt innehåll som inte når in i den utomspråkliga verkligheten (Butlin, 2021; Coelho Mollo &amp; Millière, 2023; Yildirim &amp; Paul, 2024). Forskare postulerar representationer av tokens, ord, meningar (Pavlick, 2023), distributioner över ord (Geva et al. 2021) och andra syntaktiska drag som roller och ordklasser (Rogers et al., 2020). Men vissa postulerar ibland representationer av verkliga, utomspråkliga innehåll som tid och rum (Gurnee &amp; Tegmark, 2024), relationer mellan färger (Abdou et al., 2021) och fakta av olika slag (Hernandez et al., 2024).</p><p>Mycket av denna forskning har inte varit grundad i den filosofiska litteraturen om representation, men det har nyligen gjorts några försök att anpassa och tillämpa filosofiska redogörelser för representation i biologiska organismer för att utvärdera de representationella förmågorna hos LLM:er (Butlin, 2021; Harding, 2023; Coelho Mollo &amp; Millière, 2023). Dessa diskussioner har främst utforskat redogörelser för representation baserade på informationella relationer. Men en alternativ redogörelse för representation i litteraturen, som hittills inte har fått någon djupgående undersökning när den tillämpas på LLM:er, är en redogörelse baserad på <em>strukturell korrespondens</em>.</p><p>Redogörelser baserade på strukturell korrespondens är en populär familj av redogörelser för mental representation inom medvetandefilosofi och kognitionsvetenskap (Cummins, 1996; Gładziejewski &amp; Miłkowski, 2017; Lee, 2019; O’Brien &amp; Opie, 2004; Ramsey, 2007; Shea, 2014, 2018; Swoyer,</p><p>2 ==Slut på översättning av sida 2==</p><p>==Start på översättning av sida 3== 1991). Utöver dess popularitet som en redogörelse för mental representation i allmänhet, finns det särskilda skäl att tro att detta tillvägagångssätt kan vara fruktbart att tillämpa på LLM:er. Till exempel har bevis på strukturella korrespondenser mellan interna tillstånd hos LLM:er och olika domäner i den verkliga världen använts som grund för att tillskriva representationer av världsrelaterat innehåll till LLM:er (Søgaard, 2022; 2023).¹ Vissa verkar alltså, åtminstone implicit, acceptera en syn där strukturella korrespondenser kan spela en roll i grundandet av representation.</p><p>Med utgångspunkt i den filosofiska litteraturen kommer jag att varna för att vara för snabb med att sluta sig till representation av världsrelaterat innehåll från upptäckten av en strukturell korrespondens. Efter att ha klargjort vad som krävs för att en strukturell korrespondens ska spela en genuint innehållsgrundande roll, kommer jag att argumentera för att det är en levande empirisk möjlighet att textbaserade LLM:er representerar världsrelaterat innehåll. Innan jag argumenterar för detta, kommer jag att presentera den primära utmaningen för att tillskriva världsrelaterat innehåll till LLM:er, nämligen deras textbundna natur.</p><p><strong>2. Utmaningen med textbundenhet</strong></p><p>En central utmaning för varje försök att tillskriva världsrelaterat innehåll till LLM:er är att många av dem, inklusive de som implementeras i enkla chattbotar, är <em>textbundna</em>, och har ingen direkt kontakt (via perception eller handling) med den utomspråkliga verkligheten: deras indata, utdata och träning (bortsett från mänskliga återkopplingssignaler) består enbart av text. Därmed är det vid första anblicken svårt att se hur de skulle kunna representera något annat än text.</p><p>I sin inflytelserika kritik introducerar Bender &amp; Koller (2020) ett tankeexperiment – i många avseenden likt Searles (1980) “Kinesiska rummet” – med målet att belysa att en LLM som enbart hanterar text på sin höjd kan representera <em>formella</em> egenskaper hos språk, aldrig <em>semantiska</em> egenskaper. Bender &amp; Koller ber oss att föreställa oss två engelsktalande personer, A och B, fångade på separata öde öar.</p><p>¹ Yildirim &amp; Paul (2024) utvecklar hypotesen i termer av “världsmodeller” som de definierar som “strukturbevarande, beteendemässigt verkningsfulla representationer av entiteter, relationer och processer i den verkliga världen” (s. 3).</p><p>3 ==Slut på översättning av sida 3==</p><p>==Start på översättning av sida 4== öar, som upptäcker telegrafer anslutna via undervattenskablar, vilket gör att de kan kommunicera med varandra. En superintelligent bläckfisk råkar stöta på undervattensröret och avlyssnar det, lyssnande på telegrafsignalerna som passerar igenom. Medan bläckfisken inte förstår engelska (åtminstone inte till en början), är den skicklig på att upptäcka statistiska mönster. Den börjar lägga märke till regelbundenheter i utbytet – vissa signaler förekommer i liknande sammanhang – och bygger upp ett enormt minne av dem. En dag bryter bläckfisken röret och infogar sig själv i konversationen. Nu skickas meddelandena från A inte längre till B, utan de skickas till bläckfisken, som, baserat på sin kunskap om de mest sannolika svaren på vissa signalmönster, kan skicka tillbaka meddelanden som liknar de sorters saker B skulle ha svarat.</p><p>Intuitionen som detta tankeexperiment framkallar, menar Bender &amp; Koller, är att bläckfisken, trots sin förmåga att generera signaler som verkar vara förnuftiga svar på mottagarens meddelanden, inte har någon förståelse för <em>betydelsen</em> av de meddelanden den skickar. Trots allt, medan A och B kan använda telegrafmeddelanden för att referera till olika objekt, har bläckfisken ingen erfarenhet av dessa objekt och, ännu viktigare, inget sätt att koppla dessa objekt till meddelandena. Till exempel används ett telegrafmeddelande som nämner “kokosnötter” av A för att referera till kokosnötter. Men bläckfisken – så lyder intuitionen – har inget sätt att representera kokosnötter. Textbundna LLM:er, argumenterar Bender &amp; Koller, är som bläckfisken i detta tankeexperiment. Eftersom de endast har tillgång till formella symboler och deras statistiska relationer, kan de inte representera de verkliga världsentiteter som vi använder dessa symboler för att representera.²</p><p>Bläckfisktankeexperimentet är mest kraftfullt (åtminstone vid första anblicken) om vi antar att kausala eller informationella relationer till den yttre världen är de enda medlen för att grunda världsrelaterat representationellt innehåll. Textbundna LLM:er har inte rätt sorts kausala/informationella relationer till utomspråkliga entiteter och kan därför, enligt argumentet, inte</p><p>² Strikt sett är Bender och Koller intresserade av frågan om LLM:er förstår naturliga språkuttryck, snarare än vår fråga – om de har interna representationer av verkligt (icke-lingvistiskt) innehåll. Men tankeexperimentet väcker analoga utmaningar för representationsfrågan.</p><p>4 ==Slut på översättning av sida 4==</p><p>==Start på översättning av sida 5== representera utomspråklig verklighet.³ Vissa författare har pekat på strukturella korrespondenser just för att de verkar erbjuda ett sätt att svara på Bender och Kollers tankeexperiment (Søgaard, 2023). Nedan kommer jag dock att visa att de som vill åberopa strukturell representation för att grunda världsrelaterat innehåll också måste brottas med en version av textbundenhetsbekymret.</p><p><strong>3. Redogörelser baserade på strukturell korrespondens</strong></p><p>Redogörelser för mental representation baserade på strukturell korrespondens hävdar att representationer grundas (åtminstone delvis) i en morfism, likhet eller spegling mellan en uppsättning interna tillstånd eller vehiklar (t.ex. i en organisms hjärna) och en uppsättning externa entiteter (t.ex. ute i världen). Den relevanta notionen av strukturell korrespondens är en relationsbevarande avbildning: det finns ett sätt att tilldela interna vehiklar till externa entiteter så att ett mönster av relationer bland de förra bevarar ett mönster av relationer bland de senare.⁴ Enligt sådana redogörelser är det som gör en relationell struktur <em>R</em> till en representation av en annan relationell struktur <em>S</em> (åtminstone delvis, och för åtminstone vissa representationer) det faktum att <em>R</em> bär en strukturell korrespondens till <em>S</em>. Klausulen “för åtminstone vissa representationer” i detta påstående tillåter starkare synsätt (de som anser att strukturell korrespondens är den <em>enda</em> livskraftiga representationsgrundande relationen) och svagare synsätt (de som anser att andra drag eller relationer, såsom korrelation, också kan grunda innehåll).⁵ Klausulen “åtminstone delvis” fångar det faktum att teoretiker sällan anser att existensen av</p><p>³ En anonym granskare föreslår att en LLM kan ha indirekta kausal-informationella relationer till verkliga objekt: dess interna tillstånd kan korrelera med förekomster av ord (t.ex. “kokosnöt”) som i sin tur korrelerar med förekomster av de entiteter som dessa ord refererar till (närvaron av en kokosnöt i talarens närhet). Det ligger utanför denna artikels räckvidd att fullt ut utvärdera utsikterna för kausal-informationell grundning av innehåll i LLM:er. Men ett betydande hinder för att utveckla detta förslag är det faktum att förekomster av ord ofta är radikalt frikopplade från förekomster av deras referenter: vi talar ofta om frånvarande, förflutna, framtida, möjliga, falska, abstrakta och fiktiva scenarier. ⁴ Se O’Brien och Opie (2004) och Shea (2018, s. 117) för försök att formalisera notionen av strukturell korrespondens. ⁵ För att markera denna distinktion skiljer Nirshberg (2023) mellan “stark strukturalism” och “moderat strukturalism”.</p><p>5 ==Slut på översättning av sida 5==</p><p>==Start på översättning av sida 6== en strukturell korrespondens är <em>tillräcklig</em> för att göra något till en representation eller för att fastställa dess innehåll. Strukturella korrespondenser är allestädes närvarande, och därmed skulle en redogörelse som endast åberopade strukturella korrespondenser övergeneralisera hopplöst, och i praktiken klassificera vilken struktur som helst som en representation och tillskriva dem radikalt obestämda innehåll.⁶</p><p>Det finns viss oenighet bland teoretiker om vilka ytterligare villkor som måste uppfyllas för att en strukturell korrespondens ska spela en genuint innehållsgrundande roll. Det finns dock en bred enighet om den allmänna formen av dessa villkor: som en första approximation måste den strukturella korrespondensen <em>användas eller utnyttjas</em> av systemet (eller organismen) på ett sätt som förklarar systemets framgångsrika utförande av någon uppgift eller förmåga.⁷ Lee och Calder sammanfattar denna allmänna konsensus (här refererar “S-representation” till den typ av representation baserad på strukturell korrespondens vi har diskuterat):</p><blockquote><p>S-representationer förklarar när följande villkor gäller: (1) ett system utför en uppgift (t.ex. navigerar till en målplats); (2) resultatet av den uppgiften beror på en mekanism med en komponent som strukturellt liknar uppgiftsrelevanta objekt (t.ex. en kognitiv karta); (3) framgång beror på graden av strukturell likhet mellan mekanismkomponenten och dessa objekt (t.ex. de topografiska dragen i en råttas lokala miljö). (Lee &amp; Calder 2023)</p></blockquote><p>Kanske det mest kända exemplet på en sådan representation i litteraturen är systemet med platsceller i däggdjurs hippocampus (O’Keefe &amp; Nadel 1978; O’Keefe &amp; Burgess 1996). Varje platscell är företrädesvis aktiv när djuret befinner sig på en viss känd plats och mönstren av</p><p>⁶ För diskussioner om denna utmaning se (Godfrey-Smith, 1996, ss. 184–187; Isaac, 2013; Shea, 2013). En oro över trivialiteten hos isomorfism har uppstått i olika debatter, åtminstone så tidigt som Newman (1928). ⁷ Utnyttjbara relationer i kontexten av representation diskuteras i (Godfrey-Smith, 2006). Många förespråkare för redogörelser baserade på strukturell korrespondens har åberopat utnyttjbarhet (Shea 2018; Gładziejewski &amp; Miłkowski, 2017; Isaac, 2013; Williams &amp; Colling, 2018). Andra åberopar relaterade begrepp som tolkning (O’Brien &amp; Opie, 2004) och handlingsvägledning (Gładziejewski, 2015, 2016). Se (Lee, 2019; Nirshberg, 2023) för diskussion.</p><p>6 ==Slut på översättning av sida 6==</p><p>==Start på översättning av sida 7== koaktivering mellan platsceller i en råttas hippocampus korresponderar strukturellt med spatiala relationer mellan de platser som aktiverar dessa platsceller. Till exempel finns det ett sätt att mappa platsceller till platser, så att om två platser är intilliggande, kommer de korresponderande platscellerna att ha starkt excitatoriska kopplingar (och därmed tendera att ha högkorrelerad aktivitet). Den strukturella korrespondensen mellan platsceller och platser har hypotetiserats spela en roll i grundandet av det representationella innehållet hos dessa platsceller och deras koaktiveringsrelationer (Shea 2018). Till exempel föreslår flera modeller för däggdjurs ruttplanering mekanismer där platscellsnätverket tas “offline” och den strukturella korrespondensen mellan koaktiveringsrelationer och spatial närhet utnyttjas för att identifiera den kortaste vägen till en målplats (Corneil &amp; Gerstner, 2015; Khajeh-Alijani, Urbanczik, &amp; Senn, 2015; Reid &amp; Staddon, 1998).</p><p>Detta exempel involverar en strukturell korrespondens med fysiskt rum, men strukturella korrespondenser (och representationer som involverar dem) kan involvera relationella strukturer av många olika slag – exempel från kognitionsvetenskapen inkluderar representationer av sociala relationer (Park, Miller, &amp; Boorman, 2021) och kausala relationer (Tenenbaum, Kemp, Griffiths, &amp; Goodman, 2011). Och, avgörande nog, för att en strukturell korrespondens ska finnas mellan en intern (t.ex. neural) struktur och en extern struktur, behöver relationen som utgör den interna strukturen inte vara <em>exakt samma</em> relation som den som utgör den externa strukturen (t.ex. spatiala relationer som korresponderar med spatiala relationer) – strukturell korrespondens är en abstrakt form av likhet, som involverar bevarandet av ett <em>mönster av relationer</em>, snarare än att kräva en duplicering av exakt samma relationstyp.</p><p>I det följande kommer jag inte direkt att argumentera för livskraften hos strukturell korrespondens som en redogörelse för mental representation (även om min framställning förhoppningsvis i viss mån kommer att motivera sådana redogörelser). Jag kommer inte heller att upprepa argument för legitimiteten och värdet av representationell förklaring i allmänhet. Jag kommer att anta, för argumentets skull, att en välutvecklad teori om representation baserad på strukturell korrespondens (t.ex. en som bygger in ett robust utnyttjandekriterium) kan användas för att fastställa existensen och innehållet hos representationer</p><p>7 ==Slut på översättning av sida 7==</p><p>==Start på översättning av sida 8== åtminstone i vissa fall. Omfattningen av denna artikel kommer att vara att tillämpa angreppssättet med strukturell korrespondens på LLM:er, och se om vi hittar bevis för representationer med världsrelaterat innehåll.⁸</p><p><strong>4. Tillämpning av redogörelsen på LLM:er</strong></p><p><strong>4.1. Bevis för strukturella korrespondenser mellan LLM:er och världsliga domäner</strong></p><p>Ett vanligt verktyg i analysen av artificiella neuronnät, inklusive LLM:er, är att konceptualisera den möjliga aktiviteten i ett givet lager av ett nätverk i termer av en <em>tillståndsrymd</em> (ibland även kallad en “inbäddningsrymd”). Aktiviteten hos varje enskild enhet definierar en dimension eller axel i den rymden. (Så, ett lager med n enheter kan karaktäriseras i termer av en n-dimensionell tillståndsrymd). Aktiveringsmönstret (aktiveringsvektorn) över ett lager vid en viss tidpunkt kan karaktäriseras som en <em>punkt</em> i den tillståndsrymden, det vill säga en position längs var och en av de n dimensionerna. Det finns många olika relationer mellan punkter eller regioner i en tillståndsrymd, t.ex. kan två punkter vara <em>närmare</em> varandra än två andra. Eller en viss region kan vara en <em>delmängd</em> av en annan region.</p><p>Verktyg som Representational Similarity Analysis (Kriegeskorte, Mur, &amp; Bandettini, 2008) och “sondningsmetoder” (Belinkov 2022) har avslöjat strukturella korrespondenser mellan, å ena sidan, de olika interna tillståndsrymderna hos LLM:er och, å andra sidan, relationella strukturer utanför nätverket. Jag kommer att överväga fyra exempel på strukturella korrespondenser från den empiriska litteraturen. För det första har det länge observerats, även i enklare språkmodeller, att för enskilda ord som indata tilldelas ord med liknande betydelser till närliggande punkter i aktiveringsrymden.⁹ Till exempel kan vektorerna som genereras för indata “land”, “stad” och “city” vara</p><p>⁸ Jag kommer inte heller att argumentera mot alternativa redogörelser för representation, såsom informationsteoretiska angreppssätt. Det kan finnas en pluralitet av legitima representationsgrundande villkor, så genom att visa att LLM:er kan eller inte kan representera verkligt innehåll enligt ett angreppssätt baserat på strukturell korrespondens kommer jag inte att ha visat att de kan eller inte kan det enligt något alternativt angreppssätt. De som förblir skeptiska till strukturell korrespondens som ett angreppssätt på representation kan ändå finna diskussionen användbar för att klargöra konsekvenserna av dessa redogörelser när de tillämpas på LLM:er. ⁹ Aktiveringsrymder för ord i LLM:er kallas ofta “inbäddningar” (embeddings).</p><p>8 ==Slut på översättning av sida 8==</p><p>==Start på översättning av sida 9== grupperade nära varandra i denna tillståndsrymd, medan vektorerna genererade för “inkomst”, “betalning” och “kostnader” bildar ett annat kluster, en bit från det första klustret. Det verkar alltså finnas en strukturell korrespondens mellan, å ena sidan, närhet i LLM:s aktiveringstillståndsrymd och, å andra sidan, likhet på nivån av ordbetydelse. Utöver ren likhet korresponderar vissa vektorförskjutningar mellan punkter i LLM:ers interna aktiveringsrymder med specifika relationer mellan dessa ord. Till exempel, i ett tidigt fynd med en språkmodell baserad på ett rekurrent neuronnät, visade Mikolov et al. (2013) att man via vektoraritmetik grovt kunde återfå vektorn som motsvarar “drottning” genom att subtrahera vektorn för “man” från vektorn för “kung” och sedan addera vektorn för “kvinna”.</p><p>I en mer nyligen genomförd studie undersökte Abdou och kollegor (2021) aktiveringsmönster i LLM:er (BERT, RoBERTa och ELECTRA) tränade på naturligt språkdata, och upptäckte strukturella korrespondenser mellan aktiveringsrymderna i dessa modeller och perceptuella färgrymder. De jämförde aktiveringsrymden som inducerades genom att ge modellerna prompter med färgtermer (“färgterminbäddningsrymd”), och en rymd som kodar färger i ett 3D-koordinatsystem (“CIELAB-rymd”). Med hjälp av två analysmetoder fann de blygsamma men statistiskt signifikanta strukturella korrespondenser mellan de två rymderna. Till exempel, när de jämförde aktiveringsmönstren som inducerades efter att ha gett LLM:erna en prompt som nämner “violett”, en prompt som nämner “turkos”, en prompt som nämner “oliv” och så vidare, speglar profilen av likheter och skillnader mellan dessa aktivitetsmönster (i viss mån) profilen av likheter och skillnader mellan färgerna själva (som kodade i CIELAB-färgkoordinatsystemet) (se Figur 1). Intuitivt sett inducerar termer för liknande färger liknande aktiveringsmönster (mönster som är närmare varandra i färgterminbäddningsrymden).</p><p>9 ==Slut på översättning av sida 9==</p><p>==Start på översättning av sida 10== CIELAB BERT, kontrollerad kontext [Bild som visar två 3D-punktmoln. Det vänstra är märkt “CIELAB” och det högra “BERT, controlled context”. Båda molnen visar en liknande tredimensionell arrangemang av färgade punkter, vilket illustrerar en mappning mellan färgrymden och modellens aktiveringsrymd.] <strong>Figur 1:</strong> Vänster: Färgorientering i 3D CIELAB-rymd. Höger: linjär mappning från BERT färgterminbäddningar till CIELAB-rymden. (från Abdou et al. 2021)</p><p>I en annan studie identifierade Gurnee &amp; Tegmark (2023) strukturella korrespondenser mellan LLM:ers aktiveringsrymder och temporala och spatiala strukturer. De tränade LLM:er (LLaMa-2, Pythia) på ett skräddarsytt dataset som inkluderade text som beskrev datum och platser för världshändelser och landmärken. Modellerna, när de väl var tränade, kunde producera ganska korrekta svar på frågor om tider och platser (inklusive generalisering till platser och tider som saknades i träningsdatan). Dessutom, med hjälp av en linjär sond (ett separat klassificeringsnätverk tränat för att förutsäga egenskaper hos indata från interna aktivitetsmönster i LLM:erna), kunde experimentledarna avkoda platser och tider från aktiviteten i nätverken på vissa lager. Viktigt för våra ändamål, aktiviteten verkade vara organiserad längs särskilda dimensioner, så att när en prompt nämnde en viss händelse, skulle aktiviteten längs den dimensionen vara mellanliggande mellan aktiviteten orsakad av prompter som nämnde tidigare händelser och senare händelser. Med andra ord verkade det finnas en strukturell korrespondens mellan aktiviteten längs vissa dimensioner och den temporala ordningen av händelser (en liknande effekt sågs för spatiala relationer).</p><p>10 ==Slut på översättning av sida 10==</p><p>==Start på översättning av sida 11== Vad ska vi göra av dessa strukturella korrespondenser? Ibland verkar det som om forskare tar existensen av en sådan korrespondens som tillräcklig för att grunda representation, eller så är de åtminstone inte explicita om vilka ytterligare villkor de anser måste vara uppfyllda. Denna slutsats från strukturell korrespondens till representation skulle dock vara förhastad. Som nämnts ovan är strukturella korrespondenser billiga, och därmed kan inte enbart existensen av en strukturell korrespondens grunda representation, utan att trivialisera begreppet representation. För att rättfärdiga påståendet att dessa observerade strukturella korrespondenser spelar en roll i att grunda representation av verkligt innehåll, måste vi avgöra om dessa korrespondenser <em>utnyttjas</em>.</p><p>För att skärpa utmaningen, notera att när vi observerar en korrespondens mellan interna tillstånd i en LLM och någon världslig struktur, finns det minst två alternativa förklaringar till denna korrespondens, som konkurrerar med påståendet att LLM:en representerar den världsliga strukturen. För det första, med tanke på liberaliteten i relationen för strukturell korrespondens (diskuterad i föregående avsnitt) kan korrespondenserna mellan interna tillstånd och externa tillstånd vara rent tillfälliga och inte erbjuda någon förklarande insikt i LLM:ens beteende. För det andra, och mer subtilt, kan en korrespondens med en struktur <em>S</em> vara en artefakt av det faktum att <em>S</em> strukturellt korresponderar med någon ytterligare struktur <em>T</em>, där korrespondensen med <em>T</em> mer direkt förklarar systemets beteende.</p><p>Denna sista möjlighet är viktig givet den nuvarande dialektiken: vår fråga är om textbundna LLM:er kan representera <em>verkligt</em> innehåll, i motsats till enbart lingvistiska objekt, egenskaper och relationer. Anta att vi hittar bevis på en strukturell korrespondens mellan interna tillstånd i en LLM och en struktur i den verkliga världen (såsom likhetsrelationer mellan färger). Det kan vara så att det som verkligen pågår är att LLM:en utnyttjar en strukturell korrespondens mellan sina interna tillstånd och statistiken för naturliga språkfärgtermer (t.ex. mönstren för användning av termer som “turkos” och “marinblå”, inklusive vilka andra tokens de tenderar att förekomma i kontexten av). Därmed kanske den observerade strukturella korrespondensen med den verkliga färgrymden inte i sig är</p><p>11 ==Slut på översättning av sida 11==</p><p>==Start på översättning av sida 12== utnyttjad av systemet, utan kan helt enkelt vara en bieffekt av det faktum att statistiken i naturligt språk i sig självt strukturellt korresponderar med den verkliga världsstrukturen.¹⁰ Det är alltså avgörande att fastställa att en korrespondens med en verklig världsstruktur utnyttjas, om den ska grunda representationellt innehåll i LLM:er. I resten av denna artikel kommer jag att utvärdera bevisen för att dessa verkliga strukturella korrespondenser utnyttjas.</p><p><strong>4.2. Bevis på utnyttjade strukturella korrespondenser</strong></p><p>Det kommer att vara användbart att dela upp frågan om huruvida en given strukturell korrespondens utnyttjas i två aspekter.¹¹ För det första finns det en fråga om den kausala effekten av en given intern relationell struktur på nedströmsprocessering inom LLM:en själv. Kalla detta den <em>kausala känslighetsfrågan</em>. För det andra finns det en fråga om huruvida LLM:ers framgång på uppgifter beror på graden av strukturell korrespondens mellan denna interna struktur och några målentiteter. Kalla detta <em>frågan om relevans för framgång</em>.</p><p><strong>4.2.1. Kausal känslighet</strong></p><p>Känslighetsvillkoret hjälper till att sålla bort fall där en intern struktur har en korrespondens med något, men den interna strukturen har ingen kausal inverkan på processering eller produktion av beteende. För att låna ett exempel från Shea (2014), antag att neuroner skiljer sig något i färg. Det kan finnas ett sätt att mappa neuroner i en persons hjärna till några entiteter i deras miljö så att den relativa nyansen hos neuronerna bevarar en relation över dessa entiteter. Men det är inte troligt att den relativa nyansen hos neuronerna därmed <em>representerar</em> den relationen. Varför? En del av anledningen är att det är högst osannolikt att processering i hjärnan är kausalt känslig för den relativa nyansen hos neuroner; det finns ingen nedströmsprocess som (säg) tar färgskillnaden</p><p>¹⁰ Förslaget här är inte att LLM:er memorerar hela sin träningsdata och uttömmande kodar frekvensen av varje sekvens av tokens. Det är att de kan förlita sig på interna strukturer som fångar statistiska relationer mellan några viktiga delmängder av tokens. ¹¹ Jag bygger här starkt på Sheas (2018) uppackning av begreppet utnyttjbarhet.</p><p>12 ==Slut på översättning av sida 12==</p><p>==Start på översättning av sida 13== mellan två neuroner som indata och levererar en utdata baserad på den skillnaden (Shea, 2014, s. 133-4).</p><p>Behovet av att etablera kausal känslighet betonas ibland i AI-litteraturen som analyserar (förmodade) representationer i LLM:er. Till exempel spårar Meng och kollegor (2022) de nedströms kausala effekterna av interventioner på särskilda aktiveringar som en del av deras rättfärdigande för att identifiera dessa aktiveringar som vehiklar för representationellt innehåll. Viktigt nog utforskade dock inte denna specifika studie kausal känslighet för <em>relationer</em> mellan aktiveringar, vilket är det som är i fråga när man bedömer utnyttjbarheten av strukturella korrespondenser.</p><p>Utnyttjande har också diskuterats i vissa filosofiska behandlingar av representation i LLM:er. Till exempel är “användning” ett av de tre kriterierna för representation i Hardings (2023) redogörelse för representation i LLM:er – där användning operationaliseras i termer av de kausala effekterna av interventioner på kandidatrepresentationella vehiklar. Medan Hardings redogörelse fokuserar på användning eller utnyttjande av <em>informationella</em> korrespondenser, diskuterar Coelho Mollo &amp; Millière (2023) kortfattat strukturella korrespondenser i sin bredare diskussion om representationsgrundning i LLM:er. De betonar också att strukturella korrespondenser ensamma är otillräckliga för att grunda representationellt innehåll, även om de inte helt packar upp vad som skulle krävas för att etablera utnyttjande av en strukturell korrespondens.</p><p>Är processering i LLM:er kausalt känslig för interna strukturer som står i strukturella korrespondenser till utomspråkliga strukturer? Det finns vissa bevis för att den är det: Merullo, Eickhoff &amp; Pavlick (2024) undersökte hur transformatorbaserade språkmodeller löser en uppsättning uppgifter som att svara på frågan “F: Vad är Frankrikes huvudstad? S: Paris F: Vad är Polens huvudstad? S: _”. De visade att LLM:ernas svar berodde på en vektoradditionsoperation som utfördes i vissa lager av nätverket. I dessa avgörande lager transformeras aktiveringsmönstret som skickas vidare från enheterna i det föregående lagret (som kan konceptualiseras som en vektor) på ett förutsägbart sätt: en specifik vektor läggs till den ursprungliga vektorn. Efter vektoradditionsoperationen matar LLM:en ut “Warszawa”, medan modellen utan den operationen skulle mata ut</p><p>13 ==Slut på översättning av sida 13==</p><p>==Start på översättning av sida 14== “Polen”. Viktigt är att effekten av exakt samma vektoradditionsoperation verkade vara kontextuellt lämplig över många olika indata. Experimentörerna demonstrerade detta genom att ändra prompten och sedan ingripa i nätverket med vektorn extraherad från “Polen”-&gt;”Warszawa”-uppgiften. Om en vektor i ett visst lager annars skulle förbereda modellen för att producera ett landsnamn som utdata, resulterade tillägget av den kritiska vektorn i det lagret i ett lämpligt huvudstadsnamn som utdata, i stort sett oberoende av landsnamnet. Till exempel, att lägga till den till en “Kina”-vektor skulle resultera i “Beijing”, “Somalia” i “Mogadishu”, och så vidare. Vissa LLM:er verkar alltså förlita sig på en sorts enkel aritmetik på vektorer (att addera en vektor till en annan) när de löser vissa uppgifter.</p><p>Forskarnas interventioner var inte alltid framgångsrika. Ofta resulterade interventionen inte i att ändra utdatan, men den resulterade i en signifikant ökning av den reciproka rangordningen för målstadens namn (rangordningen som tilldelades den i sannolikhetsfördelningen över alla tokens, extraherad från det lagret). Och i 37 % av fallen hade interventionen ingen effekt (denna felfrekvens var lägre för andra relationella uppgifter, t.ex. att omvandla verb till deras preteritumform).¹² Så de underliggande mekanismerna här är uppenbart komplexa och i behov av ytterligare empirisk undersökning. Icke desto mindre antyder studien den typ av bevis som är relevanta för att bedöma om processering i LLM:er är känslig för en viss relation mellan vektorer.</p><p>För att utveckla denna tanke, låt oss något spekulativt utvidga den mekanism som föreslagits av Merullo och kollegor: antag att processering i vissa lager av LLM:er beräknar förskjutningen mellan punkten i aktiveringsrymden som tilldelats “Frankrike” och punkten i aktiveringsrymden som tilldelats “Paris” och sedan förskjuter punkten i aktiveringsrymden som tilldelats “Polen” på samma sätt, för att anlända till en punkt i aktiveringsrymden som producerar utdatan “Warszawa”. Om detta är korrekt, är åtminstone vissa relationer mellan vektorer inte som relationer av relativ nyans mellan</p><p>¹² Författarna diskuterar några möjliga förklaringar till dessa resultat (Merullo, Eickhoff &amp; Pavlick 2024, s. 6).</p><p>14 ==Slut på översättning av sida 14==</p><p>==Start på översättning av sida 15== neuroner – de har en verklig kausal inverkan på nedströmsberäkningar, och i slutändan på LLM:ens utdata, så de uppfyller känslighetskravet för utnyttjbara strukturella korrespondenser.¹³</p><p>Detta är bara ett illustrativt exempel på den typ av data som skulle kunna vara relevant för att besvara känslighetsfrågan. Viktigt är dock att, med tanke på att många olika strukturella korrespondenser har hävdats grunda representationellt innehåll i LLM:er, måste känslighetsfrågan besvaras från fall till fall. Bearbetning i LLM:er kan vara kausalt känslig för specifika vektoraritmetiska relationer, men är de känsliga för <em>avstånd i aktiveringsrymden</em> mer generellt? Hur är det med <em>mereologiska relationer</em> i aktiveringsrymden (som att en viss region är en delmängd av en annan)? Mer empiriskt arbete behövs för att besvara dessa frågor.</p><p><strong>4.2.2. Relevans för framgång</strong></p><p>En strukturell korrespondens som är en kandidat för att grunda representationellt innehåll involverar en relation mellan två strukturer (dessa strukturer i sig konstituerade av relationer mellan vissa entiteter). På den ena sidan finns en intern struktur (t.ex. en relation mellan en uppsättning egenskaper hos ett neuronnät). På den andra sidan finns en extern struktur (t.ex. en relation mellan en uppsättning fysiska platser). I det förra underavsnittet, när vi diskuterade kausal känslighet för relationerna som utgör en intern struktur, betraktade vi bara ena sidan av denna ekvation. Även efter att ha etablerat kausal känslighet för en intern struktur, återstår frågan om den kausala känsligheten fungerar för att tillåta systemet att hantera, navigera eller respondera på strukturen i andra änden av den kandidatstrukturella korrespondensen.</p><p>Frågan om relevans för framgång är frågan om ett systems framgång i att utföra uppgifter beror på graden av strukturell korrespondens mellan en intern struktur och</p><p>¹³ Det är en berättigad fråga hur denna jämförelseoperation skulle kunna ske (jag tackar en anonym granskare för att ha tagit upp detta). Förmodligen spelar self-attention-lagren en roll, eftersom de tar två vektorer som input när de flyttar information mellan tokenpositioner. Det är dock också troligt att förskjutningen av dessa vektorer beräknas i det efterföljande multi-layer perceptron (MLP)-blocket, genom någon transformation som skiljer bidraget från attention-lagret från den ursprungliga vektorn för den tokenpositionen. Att avgöra om och, i så fall, hur modeller är kausalt känsliga för förskjutningar i inbäddningsrymden är ett jobb för framtida empirisk forskning.</p><p>15 ==Slut på översättning av sida 15==</p><p>==Start på översättning av sida 16== vissa externa förhållanden. En intern struktur, även en som intern processering är kausalt känslig för, kan korrespondera med ett antal olika saker, men endast vissa av dessa korrespondenser kommer att vara förklarande relevanta för hela systemets (t.ex. organismens eller LLM:ens) framgångsrika beteende. För att ge ett överdrivet exempel, avstånd mellan punkter i aktiveringsrymden hos neuroner i en grodas hjärna kan, genom ren slump, ha en strukturell korrespondens med avstånd mellan asteroider i en avlägsen galax. Men det som händer i avlägsna galaxer är inte grodans angelägenhet (så att säga), och därför är denna strukturella korrespondens osannolik att vara förklarande för grodans beteendemässiga framgång, även om processering i grodans hjärna är kausalt känslig för avstånd i neural aktiveringsrymd.</p><p>Yildirim &amp; Paul (2024) verkar ha något liknande i åtanke när de säger att för att kvalificera sig som en världsmodell måste en struktur i en LLM “vara beteendemässigt verkningsfull, vilket innebär att den bör möjliggöra korrekt planering och högbelönade handlingar tillbaka i den verkliga världen” (s. 2, betoning tillagd). Uppenbarligen kommer svaret på frågan om relevans för framgång för LLM:er att bero på <em>yttre</em> fakta om LLM:er – på den typ av miljö de är inbäddade i och på vilka uppgifter de uppmanas att utföra. Dessa uppgifter introducerar framgångsvillkor och begränsar därmed poolen av strukturella korrespondenser som är kandidater för att vara förklarande (vara kausalt relevanta för att uppnå framgångsrika resultat).</p><p>Hur påverkar detta möjligheten för LLM:er att representera verkliga (i motsats till lingvistiska) entiteter? Om de uppgifter ett system utför begränsar vad det kan representera, då uppstår en version av den oro som Bender och Kollers bläckfisktankeexperiment väckte igen: textbundna LLM:er utför inte uppgifter som <em>direkt</em> involverar verkliga entiteter – deras indata och utdata består enbart av text – och detta kan naturligtvis tänkas förhindra dem från att representera verkliga domäner (oavsett om deras interna tillstånd råkar strukturellt korrespondera med dessa domäner). Självklart skulle man kunna modifiera en textbunden LLM så att dess förmågor involverade att agera på eller svara på verkliga entiteter. Multimodala och förkroppsligade</p><p>16 ==Slut på översättning av sida 16==</p><p>==Start på översättning av sida 17== LLM:er ger exempel.¹⁴ Men mitt fokus i denna artikel är på <em>textbundna</em> LLM:er, som inte har någon text-omedierad tillgång till den verkliga världen.¹⁵</p><p>Trots denna utmaning kommer jag att visa att den textbundna naturen hos LLM:er inte omedelbart utesluter innehåll om den verkliga världen enligt en redogörelse för representation baserad på strukturell korrespondens: i princip kan graden av korrespondens mellan LLM:ers interna strukturer och verkliga entiteter fortfarande förklara deras framgång på textbaserade uppgifter. Att bestämma hur man bäst karaktäriserar dessa uppgifter visar sig dock vara en icke-trivial fråga.</p><p><strong>4.2.3. Vilka uppgifter utför LLM:er?</strong></p><p>LLM:ers grundläggande funktionalitet innebär att omvandla textindata till textutdata. Men hur ska vi tänka kring dessa förmågor? De kan beskrivas som, säg, att sammanfatta, översätta, besvara faktafrågor, erbjuda rekommendationer. Dessa beskrivningar, och de framgångsvillkor de implicerar, involverar att behandla indata och utdata som meningsfulla (dvs. att individualisera dem semantiskt), och för vissa uppgifter, att jämföra dessa betydelser med en extern, verklig standard (fakta i målet). Till exempel är ett bra (framgångsrikt) svar på en faktafråga (“Paris”) ett vars innehåll är lämpligt givet innehållet i indatan (“Vad är Frankrikes huvudstad?”), och hur världen faktiskt är (det faktum att Paris är Frankrikes huvudstad). Men för varje given förmåga kommer det att finnas en konkurrerande beskrivning av den förmågan i icke-semantiska termer. Till exempel skulle vi kunna beskriva en LLM som engagerad i uppgiften att förutsäga den mest statistiskt sannolika token som följer en sekvens av tokens (för ett brett spektrum av indatasekvenser). Här individualiseras indata och utdata</p><p>¹⁴ SayCan (Ahn et al., 2022) är en implementering av en LLM i en instruktionsföljande robot. När den får prompten “Jag spillde min dricka, kan du hjälpa mig?” kan systemet bryta ner denna begäran i kontextuellt lämpliga deluppgifter och sedan utföra dessa uppgifter i den verkliga världen (hitta en svamp, plocka upp den, lokalisera spillet, använda svampen för att torka upp). Ett intressant mellanfall är ett system som kan utföra handlingar i en virtuell miljö som en webbläsare, såsom Adepts ACT-1-modell (Adept 2022). ¹⁵ Detta skiljer utmaningen som behandlas i denna artikel från tillämpningar av strukturella representationsbaserade redogörelser för representation på artificiella neuronnät i tidigare filosofiskt arbete (Churchland 1998; 2012; O’Brien &amp; Opie, 2004, 2006).</p><p>17 ==Slut på översättning av sida 17==</p><p>==Start på översättning av sida 18== är individualiserade i rent formella termer, och framgång är en fråga om att spåra den faktiska statistiken för mänskligt genererat språk (snarare än att spåra några utomspråkliga fakta).</p><p>Dessa olika sätt att beskriva systemets förmågor är betydelsefulla eftersom de resulterar i olika redogörelser för vad framgång utgör, och det som är framgångsrikt i förhållande till en uppgift kan vara misslyckat i förhållande till en annan. Tänk till exempel på studien av Merullo och kollegor som beskrivs ovan i avsnitt 4.2.1. Om vi tänker på uppgiften som LLM:en utför som uppgiften att korrekt identifiera huvudstaden i ett land, då kommer framgångsrik prestanda att kräva att modellen svarar “Warszawa” när den tillfrågas om Polens huvudstad. Men antag nu att Polens huvudstad officiellt ändras från Warszawa till Kraków, men modellen inte omtränas, så att den fortsätter att svara “Warszawa”. För ett rent exempel, låt oss också anta att nyheten om förändringen av Polens huvudstad under en kort tid inte är allmänt publicerad, och de flesta människor fortsätter att (nu felaktigt) avsluta meningen “Polens huvudstad är ___” med “Warszawa”. Liksom dessa människor skulle LLM:ens svar “Warszawa” (med tanke på de nya fakta) räknas som en <em>misslyckad</em> prestation av uppgiften, enligt denna tolkning av uppgiften som modellen är engagerad i. Däremot, om modellens uppgift bara är att förutsäga det mest statistiskt sannolika nästa ordet eller token, förblir dess svar “Warszawa” framgångsrikt även efter förändringen i de geopolitiska fakta. Detta beror på att (enligt antagandet) statistiken för mänskligt producerat språk i stort sett är oförändrad, och “Warszawa” förblir det mest statistiskt sannolika svaret på den frågan.</p><p>Om frågan om huruvida en strukturell korrespondens utnyttjas beror på om den bidrar till systemets framgångsrika utförande av sina uppgifter, då kräver ett svar på denna fråga att man först bestämmer sig mellan dessa konkurrerande redogörelser för dess uppgifter (och de framgångsvillkor de implicerar). Hur kan vi göra detta?</p><p>18 ==Slut på översättning av sida 18==</p><p>==Start på översättning av sida 19== <strong>4.2.4. Träningshistorik som en determinant för framgångsvillkor</strong></p><p>När det gäller biologiska organismer är en framträdande uppfattning att framgångsvillkoren för en organisms beteende härrör från vad som är “bra för” systemet eller, relaterat till detta, hur beteenden belönas eller förstärks, i stort sett i linje med den teleosemantiska traditionen (Dretske 1988; Millikan 1984). Även om det finns olika redogörelser som erbjuds (Maley &amp; Piccinini, 2017; Piccinini, 2022; se Lee 2021 s. 822–823 för diskussion), tar ett lovande tillvägagångssätt en bakåtblickande syn och hävdar att framgångsvillkor bestäms av de faktorer som spelade en kausal roll i att stabilisera beteendemässiga dispositioner i organismens, evolutionära linjens eller systemets historia (Shea 2018).</p><p>Shea (2018) diskuterar tre typer av stabiliserande processer som kan spela denna roll: (i) evolution genom naturligt urval, (ii) bidrag till persistensen av en enskild organism eller ett system och (iii) inlärning baserad på feedback. I varje fall finns det en process som avgör vilka dispositioner som överlever och består i framtiden (antingen i en enskild organism eller i dess avkomma). Till exempel kommer naturligt urval att tendera att stabilisera, i en population, dispositioner att skaffa mat, eftersom individer som saknar denna disposition är mindre benägna att sprida sina gener (evolution genom naturligt urval). Och på tidsskalan för en enskild individ kommer en organism och dess dispositioner att överleva längre om en av dessa dispositioner är att skaffa mat (bidrag till persistensen av en enskild organism eller ett system). Inlärningsmekanismer spelar en liknande roll, vanligtvis på kortare tidsskalor: en mekanism som förstärker beteenden när de leder till högvärdiga resultat kommer att tendera att stabilisera vissa dispositioner, såsom en disposition som resulterar i att skaffa mat, över andra (inlärning baserad på feedback).</p><p>Existensen av dessa processer ger ett icke-godtyckligt sätt att skilja mellan framgångsrika resultat (för att tala metaforiskt, resultat som en organisms beteende “försöker” eller “ska” uppnå) och oavsiktliga eller misslyckade resultat. Framgångsrika resultat är de som har varit målet för stabiliserande processer; misslyckade eller oavsiktliga resultat är</p><p>19 ==Slut på översättning av sida 19==</p><p>==Start på översättning av sida 20== de som inte har varit det. Det vill säga, vi kallar ett resultat av beteende vid ett visst tillfälle framgångsrikt om det resultatet är av ett slag som, i organismens historia, hjälpte till att skapa eller förstärka dispositionen att producera just det beteendet.</p><p>Viktigt är att detta sätt att se på saken tillåter oss att vara någorlunda finkorniga i att specificera de uppgifter en organism utför. En groda kan ha en robust disposition att snärta med tungan efter små svarta flygande saker, men det är det faktum att små svarta flygande saker har tenderat att vara <em>flugor</em> (eller <em>näringsrika objekt</em>) som har stabiliserat grodors disposition att snärta efter dem med sina tungor. Därmed spelar de senare egenskaperna, men inte de förra, en roll i en kausal förklaring av dispositionens stabilisering, och ger därmed den korrekta karaktäriseringen av uppgiften grodan utför (och dess medföljande framgångsvillkor – den är misslyckad när den fångar en liten svart flygande sak som inte är en fluga, eller inte är näringsrik) (Shea 2018, s. 150–151).</p><p>Även om redogörelsen som just skisserades utvecklades främst i kontexten av biologiska organismer, föreslår den en parallell strategi för att bestämma framgångsvillkoren för en LLM:s dispositioner: vi behöver titta på de processer som leder till att dessa dispositioner stabiliseras och upprätthålls. Och den naturliga platsen att leta efter detta är i de maskininlärningsprocesser genom vilka LLM:er tränas.</p><p>För närvarande tränas de flesta LLM:er initialt genom en självövervakad maskininlärningsprocess på uppgiften <em>prediktion av nästa token</em>. En “token” avser ord eller delar av ord som behandlas som de grundläggande enheterna i textsträngar av LLM:en. (Till exempel kan “statistical relationship” delas upp i tre tokens: “stat”, “istical” och “relationship”). Detta första träningssteg för en LLM, ofta kallat “förträning”, innebär att ge nätverket en del av en textsträng från dess träningskorpus (t.ex. ett ofullständigt stycke, eller en mening med ett ord maskerat). Nätverkets utdata är en sannolikhetsfördelning över alla möjliga nästa tokens. Den högsta sannolikheten utgör dess “bästa gissning” på nästa token i sekvensen, och används för att generera utdatan. Det faktiska nästa ordet avslöjas sedan och, om den initiala gissningen var felaktig, uppdaterar inlärningsalgoritmen anslutningsvikterna i modellen som ledde till det felaktiga svaret. Processen är</p><p>20 ==Slut på översättning av sida 20==</p><p>==Start på översättning av sida 21== upprepas iterativt och, genom en process av gradientnedstigning, blir modellen bättre på uppgiften att förutsäga nästa ord.</p><p>Vissa LLM:er tränas enbart på denna självövervakade uppgift att förutsäga nästa ord (“förträning”). Andra LLM:er genomgår en ytterligare träningsprocess (“finjustering”). Till exempel genomgår OpenAI:s LLM:er (sedan GPT-3.5) en process av förstärkningsinlärning med mänsklig återkoppling (RLHF) (Ouyang et al., 2022). Par av svar på ett urval av prompter visas för mänskliga granskare, och data samlas in om granskarnas relativa preferenser för svar på varje prompt. Denna datamängd används sedan för att träna ett separat neuronnät – en belöningsmodell (eller “preferensmodell”). Belöningsmodellen tar prompt-svar-par som indata och genererar en skalär bedömning som utdata. När den väl är tränad fungerar belöningsmodellen som en proxy för mänskliga granskare och dess utdata kan därmed användas som en belöningssignal för att finjustera den ursprungliga förtränade LLM:en i en förstärkningsinlärningsprocess.</p><p>Avgörande är att de olika återkopplingssignalerna i dessa två träningskörningar motiverar olika tolkningar av de uppgifter en LLM är engagerad i.¹⁶ Vid förträning baseras måttet på modellens prestanda (“förlustfunktionen” eller “kostnadsfunktionen”) som används som återkoppling för att uppdatera modellens träningsbara parametrar, på skillnaden mellan den faktiska nästa token (i träningsexemplet) och den förutsagda nästa token (modellens utdata). Därmed stabiliserar förträning en disposition att producera statistiskt sannolika fortsättningar av en sekvens av ord. Statiskt sannolika fortsättningar av en sekvens av ord korrelerar förvisso med semantiskt lämpliga svar (åtminstone i en träningsdatamängd av tillräcklig storlek och kvalitet). Men i självövervakad förträning är den kausalt relevanta faktorn som driver uppdateringen av modellernas träningsbara parametrar, och därmed dess dispositioner, sannolikheten för ordförekomster villkorad av den föregående strängen av ord. Om, av någon anledning, den mest statistiskt sannolika fortsättningen av en viss sekvens är</p><p>¹⁶ Coelho Mollo och Millière (2023) argumenterar för en liknande slutsats.</p><p>21 ==Slut på översättning av sida 21==</p><p>==Start på översättning av sida 22== semantiskt olämplig (t.ex. falsk, osammanhängande eller inkonsekvent) skulle träningsproceduren inte registrera detta, så dispositionen att producera en sådan utdata skulle inte ha valts bort.</p><p>Däremot involverar finjustering med RLHF uttryckligen att välja svar baserat på semantiskt laddade kriterier. Till exempel kan mänskliga utvärderare ombes att betygsätta hur “hjälpsamma, ärliga och ofarliga” förtränade LLM:ers utdata är (Bai et al., 2022), och dessa utvärderingar matas sedan in i belöningssignalen som används för att finjustera modellen. Här verkar träningsprocessen stabilisera modellens utdata mot semantiskt lämpliga utdata. Till skillnad från förträning tenderar finjustering att välja bort semantiskt olämpliga utdata <em>i kraft av</em> deras semantiska olämplighet. Den semantiska (o)lämpligheten hos dess utdata driver kausalt stabiliseringen av dess dispositioner.</p><p>Enligt en redogörelse för de uppgifter som LLM:er är engagerade i, som är beroende av stabiliseringshistorik, ses LLM:er som tränats på olika sätt korrekt som att de engagerar sig i olika uppgifter på helsystemnivå. Medan rent förtränade LLM:er har funktionen eller målet att förutsäga den mest sannolika nästa token, kan de som finjusterats med RLHF och liknande processer utveckla uppgifter som involverar att producera semantiskt lämpliga (inklusive sanningsspårande) svar på prompter. Detta beror på att den semantiska lämpligheten hos deras utdata inte bara är en lyckosam bieffekt, utan spelade en historisk kausal roll (via en människomedierad träningsalgoritm) i att de har de dispositioner de nu har.</p><p><strong>4.2.5. Bidrar strukturella korrespondenser med verkliga strukturer till LLM:ers framgångsrika uppgiftsprestation?</strong></p><p>Betrakta en förtränad LLM vars uppgift på systemnivå helt enkelt är att generera sannolika nästa tokens. Det är tänkbart att ett system av detta slag uppnår sitt mål delvis genom att utnyttja strukturella korrespondenser mellan dess interna aktiveringsrymder och verkliga strukturer. Vi kan till exempel föreställa oss en modell som använder en representation av de strukturella fakta i en världslig</p><p>22 ==Slut på översättning av sida 22==</p><p>==Start på översättning av sida 23== domän som ett mellanliggande steg på vägen mot att beräkna sannolika fortsättningar av text.¹⁷ Till exempel, om indatakontexten antyder att modellen ska producera text i stil med en person med dålig geografisk kunskap, kan en sådan modell börja med en representation av de faktiska huvudstad-land-relationerna och sedan modulera sitt svar bort från sanningen, för att generera en statistiskt sannolik utdata.</p><p>Men det är också möjligt att en förtränad LLM inte involverar någon sådan mekanism. Den kan förlita sig direkt på strukturella korrespondenser till statistiska samberoenderelationer mellan en klass av ord (“Kina”, “Frankrike”, “Polen” etc.) och en annan (“Beijing”, “Paris”, “Warszawa” etc.), utan att gå via ett mellanliggande beräkningssteg vars funktion är att korrespondera med verkliga land-huvudstad-relationer. Att bara identifiera en LLM:s uppgift på systemnivå avgör alltså inte frågan om vilken av de konkurrerande strukturella korrespondenserna (som involverar verkliga strukturer, kontra rent lingvistisk-statistiska strukturer) som utnyttjas.</p><p>Hur kan vi empiriskt skilja dessa två hypoteser åt? Ett sätt att utvärdera vilken strukturell korrespondens som är förklarande för framgång är att ingripa i systemet för att modulera graden av varje strukturell korrespondens.¹⁸ Om det finns flera korrespondenser som konkurrerar om förklarande relevans, kan dessa manipuleras oberoende för att se vilken som har störst effekt på framgången (relativt de framgångskriterier som är tillämpliga på systemets utdata). Det vill säga, man skulle kunna artificiellt justera geometrin i en LLM:s aktiveringsrymd på ett lager av intresse, för att differentiellt skärpa den strukturella korrespondensen till antingen (i) den verkliga strukturen eller (ii) den lingvistisk-statistiska strukturen.¹⁹ Om den första hypotesen är sann – LLM:en förlitar sig på ett steg som utnyttjar en korrespondens till faktiska land-huvudstad-relationer – borde vi hitta ett lager där en ökning av korrespondensen med (i) har en större positiv effekt på prestandan än en ökning av</p><p>¹⁷ Jag tackar två anonyma granskare för att de pressade mig att ta itu med denna typ av möjlighet. ¹⁸ Shea (2018, s. 142-143) diskuterar den bevisvärderande rollen av interventioner på strukturella korrespondenser. ¹⁹ För att ett sådant experiment ska vara möjligt måste det vara så att (i) och (ii) inte själva är i perfekt korrespondens, så att de kan moduleras differentiellt. Detta är troligt: det är sannolikt att det finns många fall där de vanligare samförekomsterna är mellan ordpar, som “Australien”-“Sydney” och “Nigeria”-“Lagos”, som avviker från land-huvudstad-par.</p><p>23 ==Slut på översättning av sida 23==</p><p>==Start på översättning av sida 24== korrespondensen med (ii) (allt annat lika).²⁰ “Positiv effekt” här betyder att generera mer <em>sannolika</em> tokens vid utdatan över relevanta kontexter, givet att detta i detta fall är det mått mot vilket vi mäter systemets framgång.²¹</p><p>Samma test skulle kunna utföras på ett system som har finjusterats med RLHF (och därmed har uppgifter på systemnivå som involverar att generera sanningsspårande utdata). Men för dessa system (i motsats till rent förtränade system) skulle vi behöva mäta effekten av att modulera en korrespondens inte i termer av om mer sannolika svar genereras, utan om mer <em>semantiskt lämpliga, sanningsspårande</em> utdata genereras. I princip skulle de olika framgångskriterierna kunna ge olika utslag på vilken korrespondens som utnyttjas.</p><p>Ovan noterade jag att det är tänkbart att en rent förtränad LLM utvecklar mekanismer som fungerar för att korrespondera med verkliga strukturer, som ett medel för att generera statistiskt sannolika utdata. Men det är värt att notera att när det gäller system som finjusterats med RLHF, finns det ytterligare skäl att förvänta sig att de utvecklar interna strukturer vars funktion är att strukturellt korrespondera med verkliga strukturer, med tanke på att en nyckeluppgift för sådana system är att generera utdata som respekterar verkliga fakta.</p><p>En del nyligen genomfört empiriskt arbete pekar mot den typ av interventionsexperiment som behövs här. Chen och kollegor (2023) byggde vidare på Gurnee och Tegmarks undersökning av spatiala korrespondenser i LLM:er (diskuterad ovan i avsnitt 4.1). De försökte avgöra om strukturella korrespondenser med spatiala strukturer var kausalt relevanta för prestanda. Liksom Gurnee och Tegmark tränade de sonderande regressor på de interna aktiveringarna av två LLM:er (i detta fall DeBERTa-v2 och GPT-Neo). Sondens mål var att från den interna aktiveringen härleda de latituder och longituder som motsvarar den plats som namnges i modellens aktuella</p><p>²⁰ När man försvagar korrespondenserna bör vi se en större negativ påverkan för den utnyttjade korrespondensen. ²¹ Harding (2023) föreslår en liknande metodik, även om hennes fokus är på informationella korrespondenser snarare än strukturella korrespondenser. Hon argumenterar för att vi kan avgöra om en informationell korrespondens utnyttjas genom att ingripa i det interna tillståndet och se om det “försämrar prestandan” i förhållande till något “godhetsmått” (s. 13-14). Jag skiljer mig från Harding i att hon nöjer sig med att “stipulera uppgiften (snarare än att söka efter en världsrelaterad uppgift som är oberoende av mänsklig tolkning)” (s. 9).</p><p>24 ==Slut på översättning av sida 24==</p><p>==Start på översättning av sida 25== indata. Liksom i den föregående studien identifierade deras sonder interna gradienter i modellernas aktiveringsrymder som togs som kandidater för att koda spatiala variationsdimensioner (latitud och longitud). För att bedöma den kausala relevansen av dessa identifierade gradienter på modellernas prestanda, störde forskarna modellens interna aktiveringar och pressade dem längs den gradienten antingen närmare eller längre bort från de “korrekta” aktiveringarna, som bestämdes av sonden. Till exempel, om en indata nämnde Tokyo, skulle interventionen föra modellens interna aktivering närmare eller längre bort från den aktivering som sonden antydde att den borde ha, givet de faktiska koordinaterna för Tokyo. I praktiken förde de alltså en uppsättning (förmodade) representationella vehiklar in i starkare eller svagare strukturell korrespondens med en verklig struktur. Forskarna fann att störning av aktiveringarna i motsatt riktning från de (förmodade) korrekta aktiveringarna hade en stark och statistiskt signifikant negativ inverkan på modellens förmåga att ge korrekta svar på prompter av formen “<stadsnamn>ligger i landet…”.²²</stadsnamn></p><p>Även om denna studie ger en känsla av den typ av interventioner jag har i åtanke, finns det två saker att notera här. För det första är de två modellerna som används av författarna förtränade på mål av typen nästa-token-prediktion, utan förstärkningsinlärning från mänsklig feedback. Som jag argumenterade i det föregående underavsnittet betyder detta att deras framgång inte bör mätas mot de <em>sanna</em> svaren på geografiska frågor – vilket var hur Chen et al. (2023) bedömde noggrannhet – utan i förhållande till <em>sannolikheten</em> för ett givet svar. För det andra utvärderade Chen och kollegor endast interventioner på korrespondensen med den verkliga spatiala strukturen. För att bestämt fastställa att detta var den utnyttjade korrespondensen, skulle man behöva jämföra magnituden av dessa effekter med andra kandidater för utnyttjade korrespondenser, vilket kan inkludera samförekomststrukturer över klasser av ord.</p><p>²² Att störa aktiveringarna <em>mot</em> de korrekta aktiveringarna såg en mycket svagare (men fortfarande statistiskt signifikant) positiv förbättring av prestandan. Författarna gissar att “LLM:en redan var nära sin optimala kapacitet, så den utökade spatiala informationen i aktiveringen efter gradientnedstigning hjälpte inte mycket med prediktionen” (s. 10).</p><p>25 ==Slut på översättning av sida 25==</p><p>==Start på översättning av sida 26== <strong>5. Slutsats</strong></p><p>En nyckelfråga i tolkningen av LLM:er är om vi ska förstå textbundna versioner av dessa modeller som att de representerar verkliga entiteter och strukturer. Genom att tillämpa en redogörelse för representation baserad på strukturell korrespondens såg vi att enbart existensen av strukturella korrespondenser mellan en LLM:s interna tillstånd och verkliga strukturer är otillräcklig för att grunda representation av dessa verkliga strukturer – dessa korrespondenser måste utnyttjas. Trots vissa prima facie-utmaningar visade jag att textbundenheten hos LLM:er inte är ett principiellt hinder för att en LLM ska utnyttja sådana korrespondenser. Även om vi såg några antydande empiriska bevis, kräver fastställandet av om (och för vilka verkliga strukturer) denna principiella möjlighet faktiskt realiseras ytterligare empirisk undersökning: För det första krävs mer arbete för att identifiera vilka modell-interna strukturer processering i LLM:er är kausalt känslig för. För det andra, för att avgöra mellan alternativa mekanistiska hypoteser, krävs arbete för att jämföra effekterna på framgång av att modulera olika strukturella korrespondenser. Som jag har argumenterat bör sådant arbete vara noggrant med att tillämpa lämpliga framgångskriterier, givet träningshistoriken för den aktuella LLM:en.</p><p>26 ==Slut på översättning av sida 26==</p><p>==Start på översättning av sida 27== <strong>Referenser</strong></p><p>[Referenslistan lämnas oöversatt, eftersom titlar och publikationsinformation ska behållas i sitt originalspråk för att vara spårbara.]</p><p>Abdou, M., Kulmizev, A., Hershcovich, D., Frank, S., Pavlick, E., &amp; Søgaard, A. (2021). Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color. Proceedings of the 25th Conference on Computational Natural Language Learning. Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., … Zeng, A. (2022). Do As I Can, Not As I Say: Grounding Language in Robotic Affordances. arXiv Preprint. <a href="https://arxiv.org/abs/2204.01691" class="extlink extlink-icon-1">https://arxiv.org/abs/2204.01691</a> Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., … Kaplan, J. (2022). Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback. arXiv Preprint. <a href="http://arxiv.org/abs/2204.05862" class="extlink extlink-icon-1">http://arxiv.org/abs/2204.05862</a> Belinkov, Y. (2022). Probing Classifiers: Promises, Shortcomings, and Advances. Computational Linguistics, 48(1), 207–219. <a href="https://doi.org/10.1162/COLI_a_00422" class="extlink extlink-icon-1">https://doi.org/10.1162/COLI_a_00422</a> Bender, E. M., &amp; Koller, A. (2020). Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, 5185–5198. Butlin, P. (2023). Sharing Our Concepts with Machines. Erkenntnis, (88), 3079–3095. <a href="https://doi.org/10.1007/s10670-021-00491-w" class="extlink extlink-icon-1">https://doi.org/10.1007/s10670-021-00491-w</a> Chen, Y., Gan, Y., Li, S., Yao, L., &amp; Zhao, X. (2023). More than Correlation: Do Large Language Models Learn Causal Representations of Space? arXiv Preprint. <a href="https://arxiv.org/abs/2312.16257" class="extlink extlink-icon-1">https://arxiv.org/abs/2312.16257</a> Churchland, P. M. (1998). Conceptual similarity across sensory and neural diversity: The Fodor/Lepore challenge answered. Journal of Philosophy, 95, 5–32. Churchland, P. M. (2012). Plato’s camera: How the physical brain captures a landscape of abstract universals. Cambridge, MA: MIT Press. Coelho Mollo, D., &amp; Millière, R. (2023). The Vector Grounding Problem. 1–34. arXiv Preprint. <a href="http://arxiv.org/abs/2304.01481" class="extlink extlink-icon-1">http://arxiv.org/abs/2304.01481</a></p><p>27 ==Slut på översättning av sida 27==</p><p>… och så vidare för resterande referenssidor.</p></div><footer class="wrapper post__footer"><p class="post__last-updated">This article was updated on juni 30, 2025</p><div class="post__share"></div></footer><nav class="pagination"><div class="pagination__title"><span>Read other posts</span></div><div class="pagination__buttons"><a href="https://avpixlat.xyz/kvantfysik-och-livets-stora-fragor-fran-medvetande-till-mening/" class="btn previous" rel="prev" aria-label="[MISSING TRANSLATION]:   Kvantfysik och livets stora frågor: Från medvetande till mening "><span class="btn__icon">←</span> <span class="btn__text"> Kvantfysik och livets stora frågor: Från medvetande till mening</span></a></div></nav></article></main><footer class="footer"><div class="footer__inner"><div class="footer__copyright"><p>© 2025 Powered by Avpixlat</p></div></div></footer></div><script defer="defer" src="https://avpixlat.xyz/assets/js/scripts.min.js?v=c2232aa7558e9517946129d2a1b8c770"></script><script>window.publiiThemeMenuConfig={mobileMenuMode:'sidebar',animationSpeed:300,submenuWidth: 'auto',doubleClickTime:500,mobileMenuExpandableSubmenus:false,relatedContainerForOverlayMenuSelector:'.top'};</script><script>var images = document.querySelectorAll('img[loading]');
        for (var i = 0; i < images.length; i++) {
            if (images[i].complete) {
                images[i].classList.add('is-loaded');
            } else {
                images[i].addEventListener('load', function () {
                    this.classList.add('is-loaded');
                }, false);
            }
        }</script></body></html>